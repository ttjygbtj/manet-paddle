MODEL: #MODEL field
  framework: "ManetSegmentationer_Stage2" #Mandatory ["Recognizer1D", "Recognizer2D", "Recognizer3D", "BMNLocalizer"], indicate the type of network, please refer to the 'paddlevideo/modeling/framework/'.
  backbone:
    name: "DeepLab" #Optional, indicate the type of backbone, please refer to the 'paddlevideo/modeling/backbones/'.
    backbone: 'resnet'
    output_stride: 16
    num_classes: 21
    freeze_bn: False
    pretrained: "/home/lc/manet/save_step_80000/save_step_80000.pdparams"
  head:
    name: "IntVOS"
    loss_cfg:
      name: "Added_CrossEntropyLoss"
      top_k_percent_pixels: 0.15
      hard_example_mining_step: 50000
    train_bn_mom: 0.9997
    model_aspp_outdim: 256
    model_semantic_embedding_dim: 100
    model_head_embedding_dim: 256
    model_useintseg: False
    test_mode: False
    model_max_local_distance: 12
TRAIN: #TRAIN
  name: 'Manet_stage2_train_helper'
  knns: 1
  damage_initial_previous_frame_mask: True
  train_total_steps: 101000
  max_iters: 80001
TEST:
  name: 'Manet_test_helper'
  knns: 1
VALID:
  train_inter_use_true_result: True
DATASET: #DATASET field
  batch_size: 2 #Mandatory, batch size per gpu.
  test_batch_size: 2 #Optional, test batch size per gpu.
  num_workers: 0 #Mandatory, the number of subprocess on each GPU.
  train:
    format: "DAVIS2017_TrainDataset" #Mandatory, indicate the type of train dataset, please refer to the 'paddlevidel/loader/dateset'.
    file_path: "datasets/DAVIS" #Mandatory, train data index file path
  valid:
    format: "DAVIS2017_TrainDataset" #Mandatory, indicate the type of train dataset, please refer to the 'paddlevidel/loader/dateset'.
    file_path: "datasets/DAVIS" #Mandatory, train data index file path
  test:
    format: "DAVIS2017_Feature_ExtractDataset" #Mandotary, indicate the type of test dataset, please refer to the 'paddlevideo/loader/dataset'.
    file_path: "datasets/DAVIS" #Mandotary, test data index file path.


PIPELINE: #PIPELINE field
  train: #Mandotary, indicate the pipeline to deal with the training data, please refer to the 'paddlevideo/loader/pipelines/'
    transform: #Mandotary, image transform operator
      - RandomHorizontalFlip_manet:
          prob: 0.5
      - RandomScale_manet:
          scales: [ 0.75, 1, 1.25 ]
      - RandomCrop_manet:
          output_size: [ 416, 416 ]
          step: 10
      - Resize_manet:
          output_size: [ 480, 854 ]
      - ToTensor_manet:
  test: #Mandatory, indicate the pipeline to deal with the validing data. please refer to the 'paddlevideo/loader/pipelines/'
    transform: #Mandatory, image transform operator.
      - Resize_manet:
          output_size: [ 480, 854 ]
      - ToTensor_manet:
  valid:
    train_inter_use_true_result:
      transform: #Mandotary, image transfrom operator
        - Resize_manet:
            output_size: 416
        - ToTensor_manet:
    transform:
      - ToTensor_manet:

DATALOADER:
  train:
    dataloader: ManetDataLoaderStage2
    sampler:
      name: RandomIdentitySampler

OPTIMIZER: #OPTIMIZER field
  name: 'Momentum' #Mandatory, the type of optimizer, associate to the 'paddlevideo/solver/'
  momentum: 0.9
  parameter_list:
    - head: [inter_seghead]
  learning_rate: #Mandatory, the type of learning rate scheduler, associate to the 'paddlevideo/solver/'
    name: 'LambdaDecay'
    learning_rate: 0.0007
    lr_lambda: "lambda last_step: (1 - last_step / (101000 + 1)) ** 0.9"
  weight_decay:
    name: 'L1'
    value: 0.00004
  grad_clip:
    name: 'ClipGradByGlobalNorm'
    value: 5

METRIC:
  name: 'CenterCropMetric'
INFERENCE:
    name: 'Manet_Inference_helper'
    output_path: inference_results

model_name: "Manet_stage2" #Mandatory, model name.
log_interval: 50 #Optional, the interval of logger.
epochs: 6000 #int(200000 * DATASET.batch_size / 60.) #Mandatory, total epoch
log_level: "INFO" #Optional, the logger level.
resume_epoch: 0
